{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script is a web scraping program designed to collect URLs of posts related to a given keyword from Parler.com. It uses Selenium and Python to navigate the Parler website, input a keyword in the search box, select the hashtag tab, and store (if found) the first 20 URLs of posts related to the keyword in a CSV file. It then adds the keyword to each URL and closes the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize the geckodriver\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# Open the browser window to parler.com\n",
    "driver.get(\"https://www.parler.com/\")\n",
    "\n",
    "# Select the \"Sign In\" link\n",
    "signin_link = driver.find_element(By.XPATH, '/html/body/div[3]/div/div/div/div/div/div/div[2]/div[1]/div[1]/a')\n",
    "signin_link.click()\n",
    "\n",
    "# Fill in the username\n",
    "username_input = driver.find_element(By.XPATH, '/html/body/div[3]/div/div/div/div/div[2]/div[1]/div[2]/input')\n",
    "username_input.send_keys(\"Deepflow\")\n",
    "\n",
    "# Select the \"Continue\" button\n",
    "continue_button = driver.find_element(By.XPATH, '/html/body/div[3]/div/div/div/div/div[2]/div[1]/button')\n",
    "continue_button.click()\n",
    "\n",
    "# Fill in the password\n",
    "password_input = driver.find_element(By.XPATH, '/html/body/div[3]/div/div/div/div/div[2]/div[1]/div[3]/input')\n",
    "password_input.send_keys(\"#Parler-STC510\")\n",
    "\n",
    "# Select the \"Login\" button\n",
    "login_button = driver.find_element(By.XPATH, '/html/body/div[3]/div/div/div/div/div[2]/div[1]/div[7]/button')\n",
    "login_button.click()\n",
    "# Wait for page to load\n",
    "time.sleep(1)\n",
    "\n",
    "# Search for the hashtag\n",
    "search_box = driver.find_element(\"xpath\",'/html/body/div[3]/div/div/header/div/div[3]/input')\n",
    "search_box.send_keys('#cybersecurity')\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(1)\n",
    "\n",
    "# Select the hashtag tab\n",
    "driver.find_element(\"xpath\",'//*[@id=\"spa\"]/div/div/div[1]/div[2]/div/ul/li[2]/a').click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(1)\n",
    "\n",
    "# Select the #cybersecurity hashtag\n",
    "driver.find_element(\"xpath\",'/html/body/div[3]/div/div/div[1]/div[2]/div/div/div/div/ul/li[1]/a').click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(1)\n",
    "\n",
    "# Get the URLs of the first 20 posts\n",
    "urls = []\n",
    "for i in range(1, 21):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    xpath = f'/html/body/div[3]/div/div/div[1]/div[2]/div/div[{i}]/div/div[1]/div[2]/div/div[3]/a'\n",
    "    try:\n",
    "        url = driver.find_element(By.XPATH, xpath).get_attribute('href')\n",
    "        urls.append(url)\n",
    "        print(f\"Post {i} URL: {url}\")\n",
    "    except NoSuchElementException:\n",
    "        print(f\"Could not find URL for post {i}\")\n",
    "        \n",
    "# Add the keyword to each URL\n",
    "keyword = 'cybersecurity'\n",
    "urls_with_keyword = [keyword + ',' + url for url in urls]\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('parler_posts.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Keyword', 'URL'])\n",
    "    for url in urls_with_keyword:\n",
    "        writer.writerow(url.split(','))\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "print(f'{len(urls)} URLs were extracted and saved to parler_posts.csv.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
