{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bradleymclellan/stc510/blob/main/Text_Analysis_Basics_NB_Optimize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a04c00a",
      "metadata": {
        "id": "6a04c00a"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b5b4725d",
      "metadata": {
        "id": "b5b4725d"
      },
      "outputs": [],
      "source": [
        "# Load Jeopardy! data\n",
        "with open('jeopardy.json', 'r') as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from the JSON data\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "rKGkUKC5uR0r"
      },
      "id": "rKGkUKC5uR0r",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "16df777f",
      "metadata": {
        "id": "16df777f"
      },
      "outputs": [],
      "source": [
        "# Clean the data\n",
        "df['clean_question'] = df['question'].str.lower().str.replace(r'[^\\w\\s]','', regex=True).str.strip()\n",
        "df['clean_answer'] = df['answer'].str.lower().str.replace(r'[^\\w\\s]','', regex=True).str.strip()\n",
        "df['clean_question'] = df['question'].apply(lambda x: ' '.join(x.split()))  # split on whitespace and join with space\n",
        "df['question'] = df['question'].apply(lambda x: ' '.join([word for word in x.split() if word not in nltk.corpus.stopwords.words('english')]))\n",
        "df['value'] = df['value'].apply(lambda x: int(re.sub(r'[^\\d]+', '', x)) if isinstance(x, str) else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "681042c4",
      "metadata": {
        "id": "681042c4"
      },
      "outputs": [],
      "source": [
        "# Define the value categories\n",
        "df['value_category'] = np.where(df['value'].isnull(), 'unknown', np.where(df['value'] < 800, 'low_value', 'high_value'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "8sFqrkd7HNdK"
      },
      "id": "8sFqrkd7HNdK",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features using TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=20000)\n",
        "vectors = vectorizer.fit_transform(df['question'])"
      ],
      "metadata": {
        "id": "yJYjuYOHCloE"
      },
      "id": "yJYjuYOHCloE",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "01b504d6",
      "metadata": {
        "id": "01b504d6"
      },
      "outputs": [],
      "source": [
        "# Define the Naive Bayes Classifier model\n",
        "model = MultinomialNB(alpha=1.0, fit_prior=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'alpha': [0.01, 0.1, 0.5, 1, 5, 10, 50, 100],\n",
        "    'fit_prior': [True, False],\n",
        "}"
      ],
      "metadata": {
        "id": "Oupq-yOsyO9U"
      },
      "id": "Oupq-yOsyO9U",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9feeb2a",
      "metadata": {
        "id": "f9feeb2a"
      },
      "outputs": [],
      "source": [
        "# Perform GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, verbose=3, n_jobs=-1)\n",
        "grid_search.fit(vectors, df['value_category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "001044c5",
      "metadata": {
        "id": "001044c5"
      },
      "outputs": [],
      "source": [
        "# Define the Naive Bayes Classifier model with best hyperparameters\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "val_vectors = vectorizer.transform(val_df['question'])\n",
        "test_vectors = vectorizer.transform(test_df['question'])\n",
        "\n",
        "val_predictions = best_model.predict(val_vectors)\n",
        "test_predictions = best_model.predict(test_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of the model\n",
        "val_accuracy = accuracy_score(val_df['value_category'], val_predictions)\n",
        "test_accuracy = accuracy_score(test_df['value_category'], test_predictions)\n",
        "\n",
        "val_confusion_matrix = confusion_matrix(val_df['value_category'], val_predictions)\n",
        "test_confusion_matrix = confusion_matrix(test_df['value_category'], test_predictions)"
      ],
      "metadata": {
        "id": "V352tAKp9z_B"
      },
      "id": "V352tAKp9z_B",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "print('Validation accuracy:', val_accuracy)\n",
        "print('Test accuracy:', test_accuracy)\n",
        "\n",
        "print('Validation confusion matrix:\\n', val_confusion_matrix)\n",
        "print('Test confusion matrix:\\n', test_confusion_matrix)"
      ],
      "metadata": {
        "id": "r7Qd1t_e98oZ"
      },
      "id": "r7Qd1t_e98oZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the summary of the findings to a csv file\n",
        "summary_df = pd.DataFrame({\n",
        "    'Model': ['Naive Bayes Classifier'],\n",
        "    'Validation Accuracy': [val_accuracy],\n",
        "    'Test Accuracy': [test_accuracy],\n",
        "})\n",
        "summary_df.to_csv('NB_Optimize_Summary.csv', index=False)"
      ],
      "metadata": {
        "id": "aB_YYU8B-cYn"
      },
      "id": "aB_YYU8B-cYn",
      "execution_count": 74,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}