{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bradleymclellan/stc510/blob/main/Text_Analysis_Essentials_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIiEhwQakENT"
      },
      "outputs": [],
      "source": [
        "# Import the necessary modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTlS8D2mkvo4"
      },
      "outputs": [],
      "source": [
        "# Load Jeopardy! data\n",
        "with open('jeopardy.json', 'r') as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxSaWrbckynj"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame from the JSON data\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYB8gQV3k2Es"
      },
      "outputs": [],
      "source": [
        "# Clean the data to remove punctuation, stopwords, and convert to lowercase\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['clean_question'] = df['question'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
        "df['clean_question'] = df['clean_question'].apply(lambda x: [word for word in x.split() if word not in stop_words])\n",
        "df['value'] = df['value'].apply(lambda x: int(re.sub(r'[^\\d]', '', x)) if isinstance(x, str) else 0)\n",
        "df['value_category'] = pd.cut(df['value'], bins=[0, 1000, np.inf], labels=['low', 'high'])\n",
        "df['high_value'] = np.where(df['value'] >= 800, 1, 0)\n",
        "df = df.drop(['category', 'air_date', 'show_number', 'value'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otxaWwIjk6zg"
      },
      "outputs": [],
      "source": [
        "# Tokenize and categorize the questions as high- or low-value\n",
        "df['tokens'] = df['clean_question'].apply(lambda x: nltk.word_tokenize(' '.join(x)))\n",
        "df['tags'] = df['tokens'].apply(lambda x: nltk.pos_tag(x))\n",
        "df['categories'] = df['tags'].apply(lambda x: [tag[1] for tag in x])\n",
        "df['question_type'] = df['categories'].apply(lambda x: 'high_value' if 'CD' in x or 'JJ' in x else 'low_value')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6uastn_mRPt"
      },
      "outputs": [],
      "source": [
        "# Define the preprocessing pipeline for the questions\n",
        "text_preprocessor = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1, 3))),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7peKXjyYWRF"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_question'], df['question_type'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rON2lDExlBKJ"
      },
      "outputs": [],
      "source": [
        "# Fit a LabelEncoder on the categories and transform the data\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUo_tFi-neZQ"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data \n",
        "X_train_preprocessed = text_preprocessor.fit_transform([' '.join(text) for text in X_train])\n",
        "X_test_preprocessed = text_preprocessor.transform([' '.join(text) for text in X_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAMW_zsflFqQ"
      },
      "outputs": [],
      "source": [
        "# Define the models used in the ensemble classifier\n",
        "svm_model = SVC(kernel='linear')\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "xgb_model = XGBClassifier(objective='binary:logistic', random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkS4NnqQYWRG"
      },
      "outputs": [],
      "source": [
        "# Fit the models on the training data\n",
        "svm_model.fit(X_train_preprocessed, y_train)\n",
        "rf_model.fit(X_train_preprocessed, y_train)\n",
        "xgb_model.fit(X_train_preprocessed, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQo2ys5_YWRG"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the training and test data\n",
        "svm_train_preds = svm_model.predict(X_train_preprocessed)\n",
        "svm_test_preds = svm_model.predict(X_test_preprocessed)\n",
        "\n",
        "rf_train_preds = rf_model.predict(X_train_preprocessed)\n",
        "rf_test_preds = rf_model.predict(X_test_preprocessed)\n",
        "\n",
        "xgb_train_preds = xgb_model.predict(X_train_preprocessed)\n",
        "xgb_test_preds = xgb_model.predict(X_test_preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjik4Nt1YWRG"
      },
      "outputs": [],
      "source": [
        "# Group the predictions together \n",
        "models = [('svm', svm_model), ('rf', rf_model), ('xgb', xgb_model)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtvbRcD9YWRG"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate the models\n",
        "scores = {}\n",
        "for model in models:\n",
        "    name = model[0]\n",
        "    clf = model[1]\n",
        "    clf.fit(X_train_preprocessed, y_train)\n",
        "    y_pred_train = clf.predict(X_train_preprocessed)\n",
        "    y_pred_test = clf.predict(X_test_preprocessed)\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "    train_report = classification_report(y_train, y_pred_train)\n",
        "    test_report = classification_report(y_test, y_pred_test)\n",
        "    scores[name] = (train_accuracy, test_accuracy, train_report, test_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCG_hnbnYWRG"
      },
      "outputs": [],
      "source": [
        "# Define the ensemble model\n",
        "ensemble = VotingClassifier(estimators=models, voting='hard')\n",
        "ensemble.fit(X_train_preprocessed, y_train)\n",
        "ensemble_predictions = ensemble.predict(X_test_preprocessed)\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
        "ensemble_report = classification_report(y_test, ensemble_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVI9AhF6YWRG"
      },
      "outputs": [],
      "source": [
        "# Print evaluation metrics for individual models and ensemble model\n",
        "for model in models:\n",
        "    name = model[0]\n",
        "    print(name.upper())\n",
        "    print(f\"{name} training accuracy: {scores[name][0]}\")\n",
        "    print(f\"{name} testing accuracy: {scores[name][1]}\")\n",
        "    print(f\"{name} classification report:\\n{scores[name][3]}\")\n",
        "    print('\\n')\n",
        "\n",
        "print(\"ENSEMBLE\")\n",
        "print(f\"Ensemble accuracy: {ensemble_accuracy}\")\n",
        "print(f\"Ensemble classification report:\\n{ensemble_report}\")\n",
        "print(f\"Ensemble precision score: {precision_score(y_test, ensemble_predictions)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBflb5m-YWRG"
      },
      "outputs": [],
      "source": [
        "# Output to a CSV a summary of the overall findings\n",
        "summary = pd.DataFrame({\n",
        "    'Model': ['SVM', 'RF', 'XGB', 'Ensemble'],\n",
        "    'Accuracy': [scores['svm'][1], scores['rf'][1], scores['xgb'][1], ensemble_accuracy],\n",
        "    'Precision': [precision_score(y_test, svm_model.predict(X_test_preprocessed)), precision_score(y_test, rf_model.predict(X_test_preprocessed)), precision_score(y_test, xgb_model.predict(X_test_preprocessed)), precision_score(y_test, ensemble_predictions, average='weighted')],\n",
        "    'Recall': [recall_score(y_test, svm_model.predict(X_test_preprocessed)), recall_score(y_test, rf_model.predict(X_test_preprocessed)), recall_score(y_test, xgb_model.predict(X_test_preprocessed)), recall_score(y_test, ensemble_predictions, average='weighted')],\n",
        "    'F1 Score': [f1_score(y_test, model[1].predict(X_test_preprocessed)) for model in models] + [f1_score(y_test, ensemble_predictions)]\n",
        "})\n",
        "summary.to_csv('Ensemble_Model_Summary.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}