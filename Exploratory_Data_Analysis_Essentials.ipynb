{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxbQrGdTapL6Afqy7poBvv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bradleymclellan/stc510/blob/main/Exploratory_Data_Analysis_Essentials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The following RQs motivated this empirical study:\n",
        "\n",
        "RQ1. Artificial Intelligence Sentiments - What are the sentiments that are being expressed about AI headline on Reddit?\n",
        "\n",
        "RQ2. Artificial Intelligence Topics - What are the main headlines that are being discussed about ChatGPT on Twitter?\n",
        "\n",
        "The following Python script scrapes the \"new\" section of the \"artificial\" subreddit on Reddit using the PRAW (Python Reddit API Wrapper) library, analyzes the sentiment of the headlines using the Natural Language Toolkit (NLTK) library, and creates visualizations of the word frequency distribution of positive and negative headlines using Matplotlib and Seaborn libraries.\n",
        "\n",
        "The code first imports the required libraries, including PRAW, NLTK, Matplotlib, and Seaborn, and then creates a Reddit instance using PRAW with a specific client ID, client secret, username, password, and user agent. The script then scrapes the \"new\" section of the \"artificial\" subreddit and stores the titles of the posts in a set called \"headlines.\"\n",
        "\n",
        "Next, the script uses the SentimentIntensityAnalyzer from the NLTK library to analyze the sentiment of the headlines. The polarity scores for each headline are stored in a list called \"results,\" which is then converted into a Pandas DataFrame called \"df.\" The DataFrame contains the polarity scores for each headline and a label (positive, neutral, or negative) based on a compound score cutoff of 0.2.\n",
        "\n",
        "The script then creates a CSV file called \"reddit_ai_headlines.csv\" containing the headline and label data. It also prints out the positive and negative headlines and their frequencies and creates a bar plot showing the percentage of positive, neutral, and negative headlines.\n",
        "\n",
        "Finally, the script processes the text of the positive and negative headlines using the NLTK tokenizer and stop words, creates frequency distributions for each set of headlines, and visualizes the word frequency distribution of the top 20 words for each set of headlines using Matplotlib."
      ],
      "metadata": {
        "id": "2cxAFMvPYJb_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bMsDR32nTOV"
      },
      "outputs": [],
      "source": [
        "# Import libraries needed for this project\n",
        "from IPython import display\n",
        "import math\n",
        "import datetime as dt\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install praw\n",
        "import praw\n",
        "from asyncpraw.models import MoreComments\n",
        "from asyncpraw.models.reddit.comment import Comment\n",
        "from asyncpraw.models.reddit.submission import Submission\n",
        "from asyncpraw.models.reddit.subreddit import Subreddit\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid', context='talk', palette='Dark2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the Reddit API using praw\n",
        "reddit = praw.Reddit(client_id='NJiWDlforFHIDkta161q_Q',\n",
        "                     client_secret='bJWi1Xx7VtSwF4Xvbtsv5XoiA3Itdw',\n",
        "                     username='bradmclellan',\n",
        "                     password='#Flusso-rouge51',\n",
        "                     user_agent='reddit devscraper by u/bradmclellan')"
      ],
      "metadata": {
        "id": "AUXdg3I2nbIF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the set for the subreddits to be scraped\n",
        "headlines = set()"
      ],
      "metadata": {
        "id": "ZRasL72ZvRMX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the /r/artificial subreddit using the API client\n",
        "for submission in reddit.subreddit('artificial').new(limit=None):\n",
        "    headlines.add(submission.title)\n",
        "    display.clear_output()\n",
        "    print(len(headlines))"
      ],
      "metadata": {
        "id": "PBbSdO10vR46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterating through the \"new\" entries in /r/artificial provides up to 1000 headlines by setting the limit to None. Reddit's 1000 result limit prevents displaying any more results without using some more sophisticated tactics. This loop can be repeated numerous times to continue adding new headlines to the collection, or a streaming variation can be used instead. For now, the focus is on the sentiment analysis of the ai headlines."
      ],
      "metadata": {
        "id": "IrswA29hdOPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Append each sentiment dictionary to a results list\n",
        "sia = SIA()\n",
        "results = []\n",
        "\n",
        "for line in headlines:\n",
        "    pol_score = sia.polarity_scores(line)\n",
        "    pol_score['headline'] = line\n",
        "    results.append(pol_score)\n",
        "\n",
        "pprint(results[:3], width=100)"
      ],
      "metadata": {
        "id": "baf5mFOvvrTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the results list into a pandas dataframe\n",
        "df = pd.DataFrame.from_records(results)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cuYSKZdwwMKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Four columns from the sentiment scoring make up the dataframe: Neu, Neg, Pos, and compound. The first three show the sentiment score percentage for each of the categories and the overall sentiment score. The value of \"compound\" is between -1 (Extremely Negative) and 1. (Extremely Positive).\n",
        "\n",
        "Posts with a compound value higher than 0.2 and less than -0.2 are regarded as positive and negative, respectively. If the compound is higher than 0.2, a positive label of 1 is created, and if it is less than -0.2, a label of -1 is created. The remainder will be 0."
      ],
      "metadata": {
        "id": "jv6sn4BDfR42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create labels for the columns\n",
        "df['label'] = 0\n",
        "df.loc[df['compound'] > 0.2, 'label'] = 1\n",
        "df.loc[df['compound'] < -0.2, 'label'] = -1\n",
        "df.head()"
      ],
      "metadata": {
        "id": "e_izgH7XwbwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataframe to a csv file\n",
        "df2 = df[['headline', 'label']]\n",
        "df2.to_csv('reddit_ai_headlines.csv', mode='a', encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "9599XW77w3rh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Review the positve and negative headlines in the dataframe\n",
        "print(\"Positive headlines:\\n\")\n",
        "pprint(list(df[df['label'] == 1].headline)[:5], width=200)\n",
        "\n",
        "print(\"\\nNegative headlines:\\n\")\n",
        "pprint(list(df[df['label'] == -1].headline)[:5], width=200)"
      ],
      "metadata": {
        "id": "Rom38PJXw-t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of positive and negatives in the dataset\n",
        "print(df.label.value_counts())\n",
        "\n",
        "print(df.label.value_counts(normalize=True) * 100)"
      ],
      "metadata": {
        "id": "4yxC13i1xJKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, the labels' original value counts are shown in the first line, while the normalize keyword-accompanied percentages are shown in the second line. Below is the plot showing the percentage of negative, neutral and postive headlines."
      ],
      "metadata": {
        "id": "OwEcreY2hDe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the percentage distibution of negative, neutral, and positive headlines\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "counts = df.label.value_counts(normalize=True) * 100\n",
        "\n",
        "sns.barplot(x=counts.index, y=counts, ax=ax)\n",
        "\n",
        "ax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
        "ax.set_ylabel(\"Percentage\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZB9N3bycxUDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There appear to be two primary causes for the abundance of neutral headlines:\n",
        "\n",
        "First, the assumption made earlier was neutral headlines are those with a compound value of between -0.2 and 0.2. The frequency of neutral headlines increases with increasing margins. Then the AI posts were divided into categories using a common lexicon. However, using an AI-specific lexicon would represent a more objective approach, but to do so, would require the hand labeling of the data or locating an existing custom lexicon."
      ],
      "metadata": {
        "id": "DukJIZr-kM-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "metadata": {
        "id": "lpclJ8_TxcTf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "kc2gG7xnzBB8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the headline list and perfrom lowercasting, tokenizing, and stopword removal\n",
        "def process_text(headlines):\n",
        "    tokens = []\n",
        "    for line in headlines:\n",
        "        toks = tokenizer.tokenize(line)\n",
        "        toks = [t.lower() for t in toks if t.lower() not in stop_words]\n",
        "        tokens.extend(toks)\n",
        "    \n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Ff-5oYz5zO-e"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect the most common words in the positive headlines\n",
        "pos_lines = list(df[df.label == 1].headline)\n",
        "\n",
        "pos_tokens = process_text(pos_lines)\n",
        "pos_freq = nltk.FreqDist(pos_tokens)\n",
        "\n",
        "pos_freq.most_common(20)"
      ],
      "metadata": {
        "id": "-qlafglkzSbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the frequency distribution of the most common words in the positive headlines\n",
        "y_val = [x[1] for x in pos_freq.most_common()]\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.plot(y_val)\n",
        "\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Word Frequency Distribution (Positive)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yY7Pq71AzYow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a logarithmic scale for the frequency distribution of positive words\n",
        "y_final = []\n",
        "for i, k, z, t in zip(y_val[0::4], y_val[1::4], y_val[2::4], y_val[3::4]):\n",
        "    y_final.append(math.log(i + k + z + t))\n",
        "\n",
        "x_val = [math.log(i + 1) for i in range(len(y_final))]\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.xlabel(\"Words (Log)\")\n",
        "plt.ylabel(\"Frequency (Log)\")\n",
        "plt.title(\"Word Frequency Distribution (Positive)\")\n",
        "plt.plot(x_val, y_final)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WNUow5-fzhjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As anticipated, a nearly straight line and a heavy tail (noisy tail). The plot mentioned above demonstrates that while the majority of words occur less frequently in our word distribution, a significant minority of them do so more frequently."
      ],
      "metadata": {
        "id": "5zCvEdqvmqt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect the most common words in the negative headlines\n",
        "neg_lines = list(df2[df2.label == -1].headline)\n",
        "\n",
        "neg_tokens = process_text(neg_lines)\n",
        "neg_freq = nltk.FreqDist(neg_tokens)\n",
        "\n",
        "neg_freq.most_common(20)"
      ],
      "metadata": {
        "id": "3-ymL0kGzvdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the frequency distribution of the most common words in the negative headlines\n",
        "y_val = [x[1] for x in neg_freq.most_common()]\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.plot(y_val)\n",
        "\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Word Frequency Distribution (Negative)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nFWfUr5Nz3w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a logarithmic scale for the frequency distribution of negative words\n",
        "y_final = []\n",
        "for i, k, z in zip(y_val[0::3], y_val[1::3], y_val[2::3]):\n",
        "    if i + k + z == 0:\n",
        "        break\n",
        "    y_final.append(math.log(i + k + z))\n",
        "\n",
        "x_val = [math.log(i+1) for i in range(len(y_final))]\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.xlabel(\"Words (Log)\")\n",
        "plt.ylabel(\"Frequency (Log)\")\n",
        "plt.title(\"Word Frequency Distribution (Negative)\")\n",
        "plt.plot(x_val, y_final)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tyoYu8LY0ED_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The incline of the negative distribution is a little smoother, but the heavy tail is unmistakably present. This conclusion here is same as the prior one shown by the positive distribution.\n",
        "\n",
        "The conclusions drawn from this research show that attitudes toward AI are generally positive, but there are a few themes that show opposition to some of its features.\n",
        "\n",
        "Literature review:\n",
        "\n",
        "Fast, E., & Horvitz, E. (2017, February). Long-term trends in the public perception of artificial intelligence. In Proceedings of the AAAI conference on artificial intelligence (Vol. 31, No. 1).\n",
        "\n",
        "This paper examines the long-term trends in public perception of AI. It contributes to the research question by exploring the factors influencing public sentiment towards AI. It also conceptualizes how AI is perceived, operationalizes the methods used to measure public sentiment, and a description of the population under examination. Additionally, the paper discusses the importance of understanding the underlying causes of public sentiment towards AI and how it affects the development of policies to address the potential ethical concerns of AI technologies.\n",
        "\n",
        "Bimber, B., & Gil de Zúñiga, H. (2020). The unedited public sphere. New media & society, 22(4), 700-715.\n",
        "\n",
        "This paper examines the unedited public sphere and its relationship to AI. It contributes to the research question by exploring how AI is perceived in the public discourse and how it influences public sentiment. The paper conceptualizes the unedited public sphere and operationalizes the methods used to access and analyze conversations about AI. It also describes the population of people engaging in those conversations and the data selection process used for the study. Additionally, it discusses the implications of AI on public discourse and how it shapes public opinion.\n"
      ],
      "metadata": {
        "id": "2HN4ERAwm7hj"
      }
    }
  ]
}